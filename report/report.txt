
## ğŸ§  Q-Learning Setup

The environment is modeled as a **Markov Decision Process (MDP)**:
- **State** = (HR zone, power zone, fatigue level, phase, target zones, slope level)
- **Action** âˆˆ {slow down, keep going, accelerate}
- **Reward** = computed based on the difference between actual and target zones and a fatigue function penalty

The Q-table is updated according to:
```
Q(s, a) â† Q(s, a) + Î± * [r + Î³ * max Q(s', a') - Q(s, a)]
```

---

## ğŸ§ª Training of the q-table
Each training session is simulated second-by-second using a predefined training plan and a real `GPX` track. The athlete progresses through phases like warmup, push, recovery, and cooldown.

Hyperparameters:
- **Î± (alpha)**: learning rate
- **Î³ (gamma)**: discount factor
- **Îµ (epsilon)**: exploration rate

> Fine-tuning was performed across multiple (Î±, Î³, Îµ) combinations to maximize accumulated reward.

---

## ğŸ—ºï¸ Environment

- **Athlete Profiles**: [elite, runner, amateur] â€” defined by HR max/rest, FTP, weight, fitness level
- **Training Plans**: [fartlek, endurance, progressions, recovery]
- **Track Data**: GPX-derived elevation converted to slope per second

---

## ğŸ›°ï¸ MQTT Communication

Every second, the system publishes an MQTT message with the suggested action and athlete state.

**Example payload**:

```json
{
  "minute": 183,
  "phase": "push",
  "fatigue": "medium",
  "action": "accelerate"
}
```

The MQTT subscriber (e.g., mobile app) receives and displays the guidance in real-time.

---

## ğŸ“ˆ Visualization

Training reward is plotted and saved for inspection. Sample Q-tables are stored in `/data/q-table/`.

---