
## 🧠 Q-Learning Setup

The environment is modeled as a **Markov Decision Process (MDP)**:
- **State** = (HR zone, power zone, fatigue level, phase, target zones, slope level)
- **Action** ∈ {slow down, keep going, accelerate}
- **Reward** = computed based on the difference between actual and target zones and a fatigue function penalty

The Q-table is updated according to:
```
Q(s, a) ← Q(s, a) + α * [r + γ * max Q(s', a') - Q(s, a)]
```

---

## 🧪 Training of the q-table
Each training session is simulated second-by-second using a predefined training plan and a real `GPX` track. The athlete progresses through phases like warmup, push, recovery, and cooldown.

Hyperparameters:
- **α (alpha)**: learning rate
- **γ (gamma)**: discount factor
- **ε (epsilon)**: exploration rate

> Fine-tuning was performed across multiple (α, γ, ε) combinations to maximize accumulated reward.

---

## 🗺️ Environment

- **Athlete Profiles**: [elite, runner, amateur] — defined by HR max/rest, FTP, weight, fitness level
- **Training Plans**: [fartlek, endurance, progressions, recovery]
- **Track Data**: GPX-derived elevation converted to slope per second

---

## 🛰️ MQTT Communication

Every second, the system publishes an MQTT message with the suggested action and athlete state.

**Example payload**:

```json
{
  "minute": 183,
  "phase": "push",
  "fatigue": "medium",
  "action": "accelerate"
}
```

The MQTT subscriber (e.g., mobile app) receives and displays the guidance in real-time.

---

## 📈 Visualization

Training reward is plotted and saved for inspection. Sample Q-tables are stored in `/data/q-table/`.

---